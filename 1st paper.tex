\documentclass[12pt]{article}
\usepackage{jcappub}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{mathtools} %For summations with limits
\usepackage{multicol} %For multiple columns
\setlength{\columnsep}{1cm}


\title{The Distribution of Vacua in Random Landscape Potentials}

\author{Low Lerh Feng,}
\author{Shaun Hotchkiss}
\author{and Richard Easther}

\emailAdd{lerh.low@auckland.ac.nz}
\emailAdd{s.hotchkiss@auckland.ac.nz}
\emailAdd{r.easther@auckland.ac.nz}

\newcommand{\re}[1]{\textcolor{blue}{[{\bf RE}: #1]}}
\newcommand{\lfl}[1]{\textcolor{red}{[{\bf LL}: #1]}}
\newcommand{\SH}[1]{\textcolor{brown}{[{\bf SH}: #1]}}
\newcommand{\sh}[1]{\textcolor{brown}{#1}}
\newcommand{\LFL}[1]{\textcolor{red}{#1}}


\affiliation{Department of Physics,\\ University of Auckland, \\Private Bag 92019,\\ Auckland, New Zealand}

\abstract{ Landscape cosmology posits the existence of a convoluted, multidimensional, scalar potential -- the  ``landscape'' -- with vast numbers of metastable minima. The huge number of minima supported by landscape potentials motivate arguments that landscape cosmology reduces tuning problems associated with a small but non-zero vacuum energy by providing a framework in which anthropic or ``environmental'' selection is plausible. Random matrices and random functions in many dimensions can be used to discuss  conceptual issues associated with landscape scenarios; here we explore the minima and vacuum energy in an $N$-dimensional Gaussian random potential. We derive a probability density for the density of minima in $N$ dimensions, showing that after rescalings its properties are fully defined by $N$ and a  single free parameter. This gives us $P(\Lambda)$, the probability density function of vacuum energies in these scenarios. \re{little more to come}}

\begin{document}

\maketitle

\section{Introduction}

Over the last two decades cosmology has developed in apparently paradoxical directions. Observationally, the rise of ``precision cosmology'' makes it possible to measure key parameters to within a few percent, setting stringent tests for the detailed evolutionary narrative given by concordance $\Lambda$CDM cosmology\cite{Planck2018,DES}. Conversely, theoretical investigations of both slow-roll inflation and string theory along with a non-zero dark energy density motivates  theoretical investigations of multiverse-like scenarios. In particular, studies of stochastic inflation \cite{Linde1986,Adshead2007} suggest that the mechanism that produces astrophysical density perturbations could also support {\em eternal inflation\/}, generating infinite numbers of  {\em pocket universes\/} \cite{Guth2001}. Likewise, studies of flux compactified string vacua points to a possible  {\em landscape\/} \cite{Susskind2003} or {\em discretuum\/} \cite{Bousso2000}   of vacua within the theory. These developments open the door to anthropic explanations of the non-zero vacuum energy density, insofar as a value that was  exactly zero could have been more plausibly explained by an unknown symmetry. 

Stochastic (or eternal) inflation implies the existence of a multiverse composed of many pocket universes, but this does not require that the ``low energy" (i.e. LHC scale) physics or vacuum energy differs between pockets:  the naive quadratic inflation model is potentially eternal, but has a unique vacuum.  By contrast, landscape models have multiple vacua which can, in principle, be populated by stochastic inflation or tunneling. The string landscape, built on the plethora of flux-stablilsed vacua that exist inside Calabi-Yau spaces, is well-known but  is not necessarily a unique realisation of this scenario. The complexity of the landscape  and the vast number of vacua it supports is the basis of its purported explanatory power: the number of vacua is almost uncountably large (e.g. $10^{500}$ or greater \cite{Douglas2003}) and any value of the vacuum energy can thus conceivably be realised within it. 
 
The detailed properties of any possible landscape are almost entirely unknown -- it is still unclear whether any specific stringy construction  realises  the $SU(3) \times SU(2) \times U(1)$ gauge group of the Standard Model.  More recently the Swampland Conjecture suggests that all stable minima of the theory might actually be ``underwater'' \cite{Agrawal2018},  located at negative values of $\Lambda$. If true, this would require that the cosmological dark energy was underpinned by dynamical quintessence-like evolution.  

An alternative approach to   landscape scenarios is to strip them down to their barest essence -- by realising multiverse cosmology within a {\em random\/} multidimensional ($N\sim100$ or more) potential of interacting scalar fields.\footnote{Random is used here in the  context of random function theory \cite{GRF1, GRF2, GRF3}.  We refer to random functions rather than random fields, the nomenclature often seen in the mathematical literature, to avoid confusion with the individual scalar fields that are coupled by the potential.}  In this approach the ``large $N$'' properties of the multidimensional landscape actually provides leverage that can be used to develop an understanding of its properties. The first steps in this direction were taken by Aazami and Easther \cite{Aazami2006}, investigating ensembles of Hessian matrices describing extrema in a random landscape. At  a minimum in the landscape the eigenvalues of the Hessian are all positive. For simple random matrix distributions eigenvalues are likely to be evenly distributed between positive and negative values with fluctuations away from this situation being strongly suppressed at even moderate values of $N$, suggesting that the number of minima is super-exponentially smaller than the number of saddles. However, the individual entries of Hessians matrices of random functions are correlated and therefore not drawn from identical, independent distributions,\cite{Battefeld2012,Easther2016} violating a common premise of simple random matrix theory and making minima more probable. This overall approach has now been widely pursued and has extended in a number of directions \cite{Easther2006, Frazer2011, Henry2009, Marsh2013, Agarwal2011,Yang2012,Masoumi2016,Yamada2018} This line of inquiry has also motivated studies of the properties of random matrices and random functions at large $N$.\cite{Bray2007,Dean2008,Majumdar2009,Bachlechner2014,Battefeld2012,Fyodorov2013,Masoumi2017} Similar mathematical problems arise in statistical mechanics, string theory, and complex dynamics.\cite{Fyodorov2004,Douglas2004,Douglas2006,Fyodorov2007,Fyodorov2012,Fyodorov2018,Ros2019}

%The theory of inflation is the current best explanation for certain perplexing observations of the Universe, such as the monopole problem, horizon problem, and flatness problem. Inflation is thought to be caused by a scalar field, but the exact nature of this field is shrouded in unknowns. The simplest possibility is a one-dimensional scalar field analogous to the gravitational potential; however multidimensional scalar fields are also possible. In particular, string theory predicts the existence of a highly complex, $O(100)$ dimensional scalar field known as the \emph{landscape}. The string landscape is so complex that quantitative predictions are hard to extract; as a result, most studies of the landscape assume it is a Gaussian random field

%\sh{\sout{If it moves along a path where the gradient of the potential is small, there is so-called ``slow-roll" inflation. Eventually, the universe's evolution ends in a minimum of the potential. If this is the global minimum, the end-state is stable. However, bubbles of space can tunnel from a local minimum to lower-lying vacua, rendering local minima metastable, although often with lifetimes much greater than the age of the universe.}}\SH{Do we need to write the above?} 

In the conventional picture of the multiverse, the universe exists in a local minimum of the landscape. If this local minimum has a positive vacuum energy it will contribute an apparent cosmological constant term to observers within the universe. Accordingly, we are interested in the probability of minima having potential values $V > 0$. \sh{\sout{In particular,} \SH{check}W}e show that for a region of parameter space, a random potential with $10^M$ minima (where $M$ is a number of order $\mathcal{O}(100)$) can also have $P(V > 0|{\rm min}) \ll 10^{-M}$, so that it is conceivable that these scenarios have no metastable solutions with  positive vacuum energy.

These considerations may lead to a weakly non-uniform measure on the vacuum energy  \re{more on measure problem, including definition and citations} 

Finally, the methods we use also provide a probability density for the eigenvalues at minima, allowing us to infer the shape \sh{around the minima} \re{more to come} 

We start by generalising to $N$ dimensions the Kac-Rice formalism \cite{Kac1943,Rice1945} and the  machinery developed and summarised by Bond, Bardeen, Kaiser and Szalay  \cite{BBKS} (hereafter called BBKS) for studying the statistics of Gaussian random functions. This yields $N$-dimensional integral expressions for the proportion of minima that have a given value of the potential, $V$. At $N=2$ the full integral can be evaluated analytically; at $N \lesssim 10$ we can numerically compute the full integrals; and for $N \lesssim 100$ we evaluate Gaussian approximations to the underlying integrals. The Gaussian approximations are tested against the exact numerical result for $N <10$ where they match closely. These techniques can be generalised to other, more complex questions about trajectories in these potentials, and give information about the ``shapes'' of minima which may inform analyses of tunnelling and inflation. We show that $P(\Lambda)$, the probability density function for the vacuum energy, depends on the dimensionality $N$ and a single free parameter for a Gaussian random landscape, and calculate how $P(\Lambda)$ varies with $N$. Finally, we discuss the implications of these results for landscape cosmology and notions of anthropic selection. \re{more detail here}\SH{we still use $V$ and $\Lambda$ interchangeably in the paper, without much apparent order. I would favour only using $\Lambda$ very sparingly when explicitly referring to the observed vacuum energy. Or, perhaps $\Lambda$ is only ever the value of $V$ when it is explicitly in a minimum?}

\section{Random Potentials in $N$ Dimensions}

%\sh{\sout{We begin by developing an $N$-dimensional analogue of the treatment of three dimensional random functions due to Bardeen, Bond, Kaiser and Szalay (henceforth BBKS) \cite{BBKS}, generalising their derivation and keeping their notation as far as possible.}}\SH{We're no longer following the notation and we say the rest in the paragraph just before this, in the introduction}.

We treat the potential energy function $V({\vec{\phi}})$ as a Gaussian random function over an $N$-dimensional field space. We wish to examine minima and saddles, therefore we require the probability density for the value of the potential itself and the values of its derivatives, $\eta_i = \partial V/\partial \phi^i$ and $\zeta_{ij}=\partial^2 V/\partial \phi^i\partial \phi^j$ at individual points in this field space. These variables can be grouped together into a vector ${\bf y} = [V,\eta,\zeta]$. In $N$ dimensions ${\bf y}$ has $\mathcal{N}=1+N+(N^2+N)/2$ independent components via $V$, $\eta$ and $\zeta$ respectively. $V$ is a Gaussian random function, this means that the variables in ${\bf y}$ are described by a multivariate Gaussian distribution. A general multivariate Gaussian distribution with $\mathcal{N}$ independent variables has the form
  %\sout{An $N$-dimensional scalar-valued function $V({\bf r})$ is a set of values \sh{\sout{that fill} defined at} each point of $N$-dimensional space. The function is \emph{random} if its values \sh{can be described by} a probability density function},
%
%\begin{equation}
%P[V({\bf r_1}), V({\bf r_2}), \ldots, V({\bf r_n})]dV({\bf r_1})dV({\bf r_2})\ldots dV({\bf r_n})
%\end{equation}
%
%\noindent where ${\bf r_i}$ is a position in $N$ dimensional space. This corresponds to the probability that the function takes value $V({\bf r_1}) \pm dV({\bf r_1})$ at position ${\bf r_1}$ \emph{and} $V({\bf r_2}) \pm dV({\bf r_2})$ at position ${\bf r_2}$, and so on.
%
%A function is \emph{Gaussian random} if the probability density function is an $N$-dimensional Gaussian function,
%
\begin{equation} \label{MultivariateGaussian}
\begin{split}
P({\bf y})d^\mathcal{N}{\bf y} &= \frac{e^{-Q}}{[(2\pi)^N \mathrm{det}(M)]^{1/2}} d^\mathcal{N}{\bf y} \, ,\\
Q &\equiv \frac{1}{2} \sum_{i,j}^\mathcal{N} \Delta y_i (M^{-1})_{ij} \Delta y_{j} \, .\\
\end{split}
\end{equation}
%
Here $\Delta y_i$ is the difference between the actual value and the mean value, i.e. $\Delta y_i \equiv y_i - \langle y_i \rangle$, and $M$ is the \emph{covariance matrix}, 
%
\begin{equation}
M_{ij} \equiv \langle \Delta y_i \Delta y_j \rangle.
\end{equation}
%
Averages denoted by $\langle \,\,\rangle$ are ensemble averages. We further assume that $\langle V\rangle = \langle \eta\rangle = \langle \zeta\rangle = 0$.\footnote{$\langle V\rangle=0$ can always be obtained by a constant shift in the definition of $V$. $\langle \eta \rangle = 0$ and $\langle \zeta\rangle = 0$ can be enforced by assuming $V$ is statistically isotropic in field space. An interesting path for future work would be to examine our results without this assumption of statistical isotropy.} Therefore the probability density only depends on the covariance matrix, $M$, and its inverse.

We next introduce the field space power spectrum, $P$, of the random function $V$, which will be useful for writing $M$ in a concise form. We define it here to be the Fourier transform of the correlation function of $V$, i.e.

\begin{equation}\label{powspec}
\langle V(\vec{\phi}_1) V(\vec{\phi}_2) \rangle = \xi(|\vec{\phi}_1-\vec{\phi}_2|)= \frac{1}{(2\pi)^N} \int d^Nk e^{i \vec{k} \cdot (\vec{\phi}_1-\vec{\phi}_2)} P(k).
\end{equation}
%
We have assumed $V$ is statistically homogeneous and isotropic in field space, therefore $\xi$ depends only on $|\vec{\phi}_1-\vec{\phi}_2|$ and  $P$ depends only on the magnitude of the Fourier coordinate $k$.\footnote{We work in a field space basis where the field space metric is Euclidean.} The moments of the power spectrum can be defined as
%
\begin{equation} \label{moments}
\sigma_n^2 = \frac{1}{(2\pi)^N}\int d^Nk (k^{2})^n P(k)
\end{equation}
%
This gives $\sigma_0^2=\xi(0)=\langle V^2 \rangle$.

We can differentiate equation \eqref{powspec} and then set $\vec{\phi}_1 = \vec{\phi}_2$ to get

\begin{align*}
\langle \eta_{i}\eta_{j}\rangle &= \frac{1}{(2\pi)^N} \frac{\partial}{\partial \phi_1^i}\frac{\partial}{\partial \phi_2^j} \int d^Nk e^{i \vec{k} \cdot (\vec{\phi}_1-\vec{\phi}_2)} P(k)\bigg{|}_{\vec{\phi}_1=\vec{\phi}_2}\\
&= \frac{1}{(2\pi)^N}\int d^Nk (k^i k^j) P(k)
\end{align*}
%
The integrand on the RHS is an odd function of both $k^i$ and $k^j$, therefore the integral over all $\vec{k}$ is zero unless $i=j$. Furthermore, because $k^2 = \sum_i k_i^2$, we see that $\sum_i \langle \eta_{i}\eta_{i}\rangle = \sigma_1^2$. We have assumed the field is statistically isotropic and thus $\langle \eta_{i}\eta_{i}\rangle=\langle \eta_{j}\eta_{j}\rangle$, meaning $\langle \eta_{i}\eta_{i}\rangle=\sigma_1^2/N$. Therefore $\langle \eta_{i}\eta_{j}\rangle=\delta_{ij}\sigma_1^2/N$.

%\begin{align*}
%\langle \eta_{\alpha,i}\eta_{\beta,j}\rangle = K \delta_{\alpha,\beta}\delta_{ij} &\rightarrow \delta_{ij}\langle \eta_{\alpha,i}\eta_{\beta,j}\rangle = NK \delta_{\alpha,\beta}=\sigma_1^2\\
%&\rightarrow K = \frac{1}{N}\sigma_1^2
%\end{align*}

A similar analysis holds for the second derivatives $\zeta_{ij}$, which can be used to find the rest of the elements of the covariance matrix. In terms of the moments of the power spectrum they are:

%Our analysis is an N-dimensional generalization of Appendix A in Bardeen, Bond, Kaiser and Szalay (henceforth BBKS) \cite{BBKS}. We adopt the same notation, $\eta_i \equiv \frac{\partial \phi}{\partial x^i}, \xi_{ij} \equiv \frac{\partial^2 \phi}{\partial x^i \partial x^j}$. In this notation, the correlations of a Gaussian random function at a random point in N-dimensions are:

\begin{equation} \label{corr}
\begin{split}
\langle VV \rangle &= \sigma_0^2 \\
\langle\eta_i\eta_j\rangle &= \frac{1}{N}\delta_{ij}\sigma_1^2 \\
\langle V\zeta_{ij}\rangle &= -\frac{1}{N}\delta_{ij}\sigma_1^2 \\
\langle\zeta_{ij}\zeta_{kl}\rangle &= \frac{1}{N(N+2)}\sigma_2^2(\delta_{ij}\delta_{kl}+\delta_{il}\delta_{jk}+\delta_{ik}\delta_{jl})
\end{split}
\end{equation}

\noindent and all other correlations are zero. For $N=3$ this reduces to equation A1 of BBKS \cite{BBKS}. 

%We define the following vector\SH{This small passage is the only time the notation $\alpha$ is used. I think we can get away without introducing it. I think in general the way BBKS goes from $F$ at different points in space to $F$, $\eta$ and $\xi$ is confusing and we can make it a bit clearer here if we want}:

%\begin{equation}
%\begin{split}
%\alpha = \{F,\eta_1,\eta_2,\ldots,\xi_{11},\xi_{22},\ldots,\xi_{NN},\xi_{N-1,N},\xi_{N-2,N},\ldots,\xi_{1N},\xi_{N-2,N-1},\\
%\ldots\xi_{1,N-1},\ldots,\xi_{12}\}
%\end{split}
%\end{equation}

%\noindent With this choice for $\alpha$ (equivalent to $\Delta y$ in Eq. \ref{MultivariateGaussian}), the covariance matrix $M_{ij}\equiv\langle\alpha_i\alpha_j\rangle$ and its inverse $K \equiv M^{-1}$ takes the following general form\SH{I guess this result comes from Mathematica? There must be a way of simplifying the $K_{\xi_{ij}\xi_{kl}}$ terms into one expression using $\delta_{ij}$ etc, like with $M$. Actually, this $K$ seems to never be used so I'm not sure we need to write it down. If we want to write out the inverse in all detail we should do it in terms of the variables we eventually use (i.e. $x_i$)}:

%\begin{align*}
%K_{F, F} &= \frac{\sigma_2^2}{\sigma_0^2\sigma_2^2-\sigma_1^4} \\
%K_{F, \xi_{ij}} &= \frac{\sigma_1^2}{\sigma_0^2\sigma_2^2-\sigma_1^4} \\
%K_{\eta_i,\eta_j} &= \frac{N}{\sigma_1^2}\\
%K_{\xi_{ii},\xi_{ii}} &=  \frac{N(N+2)}{\sigma_2^2} \\
%K_{\xi_{ij}, \xi_{ij}} &= \frac{N\sigma_0^2\sigma_2^2-(N+2)\sigma_1^4}{2(\sigma_1^4\sigma_2^2-\sigma_0^2\sigma_2^4)}, (i\neq j)\\
%K_{\xi_{ii}, \xi_{jj}} &= \frac{(N(N+1)-2)\sigma_1^4 - N(N+1)\sigma_0^2\sigma_2^2}{2(\sigma_1^4\sigma_2^2-\sigma_0^2\sigma_2^4)}, (i \neq j)\\
%\end{align*}

%\noindent and all other terms are zero\SH{Are there some $\delta_{ij}$ terms missing in the 2nd and 3rd lines above?}. The probability density of $\alpha_i$ is, from Eq. \ref{MultivariateGaussian}\SH{This is just eq. 2.2 repeated with $\alpha$ instead of $y$, I think there is a way of expressing all of this without so many introduced variables like $y$ $\alpha$ that are used only once each. I don't we've ever explicitly said we're considering functions/fields where $\langle F\rangle=0$ either, which we've assumed in order to write this equation.}.

%\begin{equation} \label{ProbDistrib}
%p(\alpha_i)=\frac{1}{(2\pi)^{N/2}\sqrt{\mathrm{det}M}} e^{-\frac{1}{2}\alpha K \alpha}
%\end{equation}
%
%We can usually ignore the constant prefactor since we will primarily be concerned with ratios of probabilities.

This covariance matrix is far from diagonal. Therefore, instead of using $V$, $\eta$ and $\zeta$ we wish to adopt a basis of variables that is as diagonal as possible. The $\eta_i$ variables are already diagonal, as are the $\zeta_{ij}$ terms with $i\neq j$. The $\zeta_{ii}$ variables are correlated to each other however, and in fact are also correlated to $V$. We look for a set of $N$ linear combinations of these $N$ variables that is diagonal. For this we choose

%
\begin{align}
\label{BasisTransform}
x_1 &= -\frac{1}{\sigma_2}\sum_i\zeta_{ii} \nonumber \\
x_n &= -\frac{1}{\sigma_2}\sum_{i=1}^{n-1}\zeta_{ii}-(n-1)\zeta_{nn},\,\, (2\leq n \leq N)
%\sigma_2x_3 &= -(\xi_{11}+\xi_{22}-2\xi_{33})\\
%\sigma_2x_4 &= -(\xi_{11}+\xi_{22}+\xi_{33}-3\xi_{44})\\
%\ldots
\end{align}
%

%\begin{equation}
%x_1 = \sum_{i=1}^N \lambda_i \, , \qquad x_n = \sum_{i=1}^{n-1} \lambda_i  -(n-1) \lambda_n\, , n\geq 2\, . 
%\end{equation} 

The $x_n$ here are analogous, but not identical, to BBKS's $x, y, z$. Following BBKS, we also rescale $V$ and introduce $\nu = V/\sigma_0$. With this choice of basis, the new nonzero correlations are, from equation \eqref{corr}:
%
\begin{eqnarray}
  \langle\nu^2\rangle &=& 1 \nonumber\\
  \langle x_1^2\rangle&=&1 \\
  \langle\nu x_1\rangle &=& \gamma \nonumber\\
  \langle x_n^2 \rangle &=& \frac{2n(n-1)}{N(N+2)},\,\, (2\leq n \leq N) \nonumber
\end{eqnarray}
%
\noindent where $\gamma = \sigma_1^2/(\sigma_2 \sigma_0)$. The only non-diagonal correlation left is between $\nu$ and $x_1$.

The $Q$ factor in equation \eqref{MultivariateGaussian} can now be calculated and takes the form

\begin{equation} \label{Q}
\begin{split}
2Q = \nu^2 + \frac{(x_1-\gamma \nu)^2}{1-\gamma^2}+\sum_{n=2}^N\frac{N(N+2)}{2n(n-1)}x_n^2 + \frac{N \pmb{\eta}\cdot \pmb{\eta}}{\sigma_1^2} + \sum_{i,j;i > j}^N\frac{N(N+2)(\zeta_{ij})^2}{\sigma_2^2}
\end{split}
\end{equation}
%

This is the equivalent of BBKS equation (A4) for $N$-dimensions, and our slightly different definition of the $x_n$ variables. Note the first two terms remain constant for all $N$, but the remaining terms vary.

Via equations \eqref{Q} and \eqref{MultivariateGaussian} we have, in as diagonal a form as possible, the full probability density function for the function, $V$, and its first two derivatives at an arbitrary point in field space. We are interested in finding probabilities about the \emph{minima} of $V$. The probability that an arbitrary point is a minima is vanishing. Therefore, we must instead examine the \emph{number density} of minima instead. To do this we first need the number density of extrema (i.e. points where $\eta=0$), which we then restrict to minima.

The process of converting the probability density into a number density of extrema is well described in 3D in BBKS \cite{BBKS} (section III a). The only difference for us is that BBKS had a field $F$ that exists in a 3D physical space ${\bf r}$, whereas we have a function $V$ existing in an $N$-dimensional field space, $\phi$. Otherwise it is the same and we get
\begin{equation}
n_{\rm ext}(\phi) \,\,{\rm d}^N \phi= |{\rm det}\,\, \zeta(\phi)| \delta^{(N)}\left[ \eta(\phi)\right] {\rm d}^N \phi.
\end{equation}

The number density of extrema, $n_{\rm ext}(\phi)$, is itself a random variable. For an actual realisation of the function it can and would vary in field space. It would be interesting for future work to study this variable and to construct the probability of minima from it directly; however it is simpler to work only with the \emph{expected} number density, which is simply the ensemble average of this variable, i.e.

\begin{equation} \label{NumberDensity}
\langle n_{\rm ext}(\phi)  \rangle= \int |{\rm det}\,\, \zeta(\phi)| P(V,\eta=0,\zeta)dV d\zeta
\end{equation}
where for notational convenience we have dropped the ${\rm d}^N \phi$ from each side.

Finally, we need to restrict this integral to consider only the extrema that are also local minima. It is easiest to do this by considering the eigenvalues, $\lambda_i$ of the Hessian of the landscape $-\zeta$ and restricting them to be negative.\footnote{Note there is an alternative convention where the eigenvalues are defined with the opposite sign, in which case \emph{positive} eigenvalues correspond to minima.} The Hessian is always symmetric by definition. We can further rotate it such that it is diagonal, in which case the diagonal elements are its eigenvalues. In order to perform this rotation, we need to compute the Jacobian. In 2D and 3D, the Jacobian of the transformation matrix can be calculated explicitly as $d \zeta = \prod_{i \neq j}^N |\lambda_i - \lambda_j|(\prod_k d\lambda_k) d \Omega$. Here ``$d\Omega$'' represents the $(N^2-N)/2$-dimensional integral measure of the set of rotation angles. The same result holds in $N$ dimensions. A proof of this result in $N$ dimensions is present in section V of \cite{Easther2016}.\footnote{The proof in \cite{Easther2016} is based on the 3D proof in BBKS and corrects a typo in that proof. Unfortunately it introduces some new typos, but the proof itself is robust. \lfl{Lol! I love this sentence}} The function $V$ and its derivatives are statistically isotropic. Therefore each set of rotation angles is as probable as any other, and the integral over all the rotation angles must give just a constant. Once this rotation is done, many simplifications occur: $\zeta_{ii}=-\lambda_i$, $\zeta_{ij}=0$ and $|{\rm det} \zeta| = \prod_i |\lambda_i|$. Accordingly, the factor $|\rm{det} \zeta(\phi)|$ in Eq. \ref{NumberDensity} can be expressed as $A \prod_i |\lambda_i| \prod_{i \neq j}^N |\lambda_i - \lambda_j|(\prod_k d\lambda_k)$, where $A$ is a constant.


%\SH{I think this bit of the paragraph is not explained well}while the off-diagonal $\zeta_{ij}$ terms can be Euler-rotated away by choosing an appropriate set of axes \SH{I don't know what this means.}. The $\xi$ matrix is already symmetric (by rotational symmetry), and Euler rotation turns it diagonal.\cite{Goldstein}

%The upshot is that $\zeta$ becomes diagonal and the individual elements of $\zeta$ are the eigenvalues $\lambda_i = -\zeta_{ii}$ of the Hessian. We are interested in minima, hence we demand all second derivatives to be positive, i.e. all eigenvalues to be \sh{negative}.\footnote{Note there is an alternative convention where the eigenvalues \sh{are defined with the opposite sign, in which case} \emph{positive} \sh{eigenvalues} correspond to \sh{minima}}

We have all the ingredients we now need, but we take one last step which is to enforce an ordering of the eigenvalues. This avoids having a multimodal integrand and allows us to ask questions about, e.g. the smallest or largest eigenvalue. We define the ordering of the eigenvalues as $\lambda_1 \leq \lambda_2 \leq \lambda_3 \ldots \leq 0$ \sh{so} our boundary conditions become $x_1\leq x_N\leq x_{N-1} ... \leq x_2 \leq 0$ because of the different choice of variables. This leads to significantly simpler boundary conditions than earlier choices for diagonalising the covariance matrix, for example in 3D our conditions are $x_1\leq x_3 \leq x_2 \leq 0$ (compare BBKS's equation between Eq. (A14) and Eq. (A15)).

This yields the following expression for the expected number density of minima (the Jacobian of the transformation between $\lambda_i$ and $x_n$ is just a constant factor):
%
\begin{equation} \label{DensityOfPeaks}
\langle n_{{\rm min}} \rangle = A \int_{\lambda_1 \leq \lambda_2 \leq \lambda_3 \ldots \leq 0} G \times e^{-Q} d\nu \left(\prod_n dx_n\right)
\end{equation}
%
\noindent where $G$ has the form\footnote{$G$ can also be expressed in terms of $x_n$ using the inverse transform of Eq. \ref{BasisTransform}.}

\begin{equation}
G = \left(\prod_{i}^{N} \lambda_i \right)\left(\prod_{i<j} |\lambda_i-\lambda_j|\right),
\end{equation} 
%
$Q$ is given by Eq. \ref{Q} and $A$ is a constant factor. We extract probabilities from this expression by calculating the proportion of minima satisfying various properties, hence we do not need to determine $A$ explicitly because it cancels when we take ratios to determine proportions. Evaluating this integral will be the focus of most of the remainder of this paper.

%We will find it convenient to make a further transformation to
%%
%\begin{equation}
%x_1 = \sum_{i=1}^N \lambda_i \, , \qquad x_n = \sum_{i=1}^{n-1} \lambda_i  -(n-1) \lambda_n\, , n\geq 2\, . 
%\end{equation} 
%%
%In these variables it can be shown that  
%%
%\begin{equation}
%Q = \frac{x^2-2\gamma x \nu+\nu^2}{1-\gamma^2} + \sum_{n=2}^N \frac{N(N+2)}{2n(n-1)}x_n^2  \label{Qx}
%\end{equation}
%%
%The inverse transformation is $\lambda_1$,
%%
%\begin{equation}
%\lambda_1 = \frac{x_1}{N} + \sum_{i=2}^N \frac{x_i}{i(i-1)} \, , \qquad
%  \lambda_n = \frac{x_1}{N} - \frac{x_n}{n} + \sum_{i=n+1}^N \frac{x_i}{i(i-1)}.
%\end{equation}
%%
%In these units the relevant region becomes $x_1\geq x_N\geq x_{N-1} ... \geq x_2 \geq 0$.

%\re{this above is can be checked for self-consistency and trimmed}

%\re{{\it can trim / compress/ move this} At this point we will briefly discuss the results obtained by Yamada and Vilenkin \cite{Yamada2018}. They have derived $p(\mathrm{min}|F, s.p.)$, while we are interested in $p(F>0|\mathrm{min})=\int^\infty_0 p(F|\mathrm{min})dF/\int^\infty_{-\infty} p(F|min)dF$. The two are related by $p(F|\mathrm{min})= \frac{p(\mathrm{min}|F, s.p.)p(F|s.p.)}{p(\mathrm{min}|sp)}$\footnote{This result is derived using Bayes theorem, as well as the fact that minima are necessarily stationary points.}.The numerator is independent of $F$, so it cancels. Therefore the two probabilities can be related with knowledge of $p(F|s.p.)$. However, this quantity is not trivial to compute. We have been able to derive an analytical formula for this in 2D (see appendix), but even in 2D the complexity is formidable.}

\section{Peak Probabilities: $V$ and $N$} \label{PeakNumbers}

\sh{\sout{Our goal is to evaluate $P(V|{\rm min})$, the probability density of vacuum energies at minima in the Gaussian random landscape.}} We are interested in the probability of finding a minimum above $V = 0$. \sh{\sout{(equivalently, the probability of finding a maximum below $\nu = 0$)} We approximate this by the ratio of the expected number density of minima above $V=0$ to the total number density of minima}, i.e. we want to calculate\footnote{\sh{We assumed earlier that $\langle V \rangle = 0$. If $\langle V \rangle \neq 0$ then this would be equivalent to a different lower bound in the following integral. We do not explore this here. \SH{We could though if we wanted?}}}
\begin{equation} \label{PminIntegral}
  P(V>0|{\rm min}) =  \frac{\int^\infty_0 d\nu \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} G \times e^{-Q} \sh{\prod_n dx_n}}{\int^\infty_{-\infty} d\nu \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} G \times e^{-Q} \sh{\prod_n dx_n}}
  \end{equation}

\SH{I don't think we need most of this below. The reason we internally wanted to set sig1 and sig0 to 1 was for the simulation code, if I remember correctly. Equation 2.8 explicitly only depends on $\gamma$ and $N$, so we can just plot things in terms of those two parameters. I think all three moments are \emph{physically} significant, but only one is relevant statistically (i.e. if we want to add in units, or scales to the problem then sig0 and sig1 are also relevant)} \lfl{I am for keep: it's in the abstract, and it's a nice piece of physical insight}

This integral depends on the value of $Q$ (Eq. \ref{Q}) and therefore on the moments of the power spectrum $\sigma_0, \sigma_1$,  and $\sigma_2$, \sh{through the single parameter $\gamma$. The allowed range of $\gamma$ is $0<\gamma<1$ because $0<\sigma_1^2<\sigma_0\sigma_2$}. \SH{could add proof if we wanted}

\sh{We now argue that of these three parameters, only one is physically significant. Given a purely Gaussian landscape we can $\sigma_0=1$ by rescaling $V$ multiplicatively, see Eq. \ref{corr}, and we can thus take $V = \nu$. Likewise, $\sigma_1$ responds to the average magnitude of the first derivatives and can be set to unity by rescaling the units of length. This leaves $\sigma_2$ as the only nontrivial parameter. It can be shown that if $\sigma_0=\sigma_1=1$, $\sigma_2>1$.\footnote{Per Ref. \cite{Yamada2018}, if $\sigma_0\sigma_2 - \sigma_1^2 < 0$, then the probability density Eq. \ref{MultivariateGaussian} cannot be normalized.} In much of what follows we follow BBKS and parameterise the landscape by $\gamma = \frac{\sigma_1^2}{\sigma_2 \sigma_0}=1/\sigma_2$ and the dimensionality $N$,  in the knowledge that $0<\gamma<1$.}\lfl{Do we include this paragraph?}

%The probability $P$ gives the (relative) likelihood of finding a minimum \re{at what point do we start talking about minima, not maxima; we should probably flip it from the start} \lfl{I don't understand, we've been using minimum since the beginning?} with value $F$ and the given values of $x_i$ which will, in turn, determine the eigenvalues of the Hessian at the minimum.  Up to an undetermined overall  constant the probability of obtaining a peak with value $F$ is given thus given by 

%\begin{equation}
%P(V) = \int dx_1 \cdots dx_N F(x) \exp{(-Q)}
%\end{equation} 

%where it is assumed that all such integrals are over $x_1\geq x_N\geq x_{N-1} ... \geq x_2 \geq 0$.
%We don't need this since it's already given in the previous section



\subsection{Landscape Heuristics}
\begin{figure}
  \centering
  
  \includegraphics[width=\linewidth]{TwoSigmas.png}
%Plot[1/2 Cos[-1.3718298377306848` x] +   1.3228756555322954` Cos[-0.9348860049921902` x], {x, -10 Pi,   10 Pi}](*Sigma2=0.9*)
%Plot[1/2 Cos[-2.5082867902473156` x] +   1.3228756555322954` Cos[-0.4940354784643969` x], {x, -10 Pi,   10 Pi}](*Sigma2=0.2*)
  \caption{We show illustrative realisations of 1D functions. The left figure has a smaller $\gamma$ \sh{\sout{(larger $\sigma_2$)}} and \sh{\sout{more “short scale” power} power over a greater range of scales}, allowing minima (maxima) to appear in significant numbers above (below) zero. When $\gamma$ is large (right figure) the spectrum is dominated by \sh{\sout{longer wavelength modes} a smaller range of modes}, and most minima are low-lying. \SH{y-axis on LHS and RHS are subtly different}}
  \label{examples1}
\end{figure}

We begin by making a qualitative exploration of the \sh{``probability density'' of our variables at minima, i.e. the integrand of the numerator in equation \eqref{PminIntegral}}. Firstly, we note that the integrand vanishes when any two \sh{\sout{``adjacent''} different} $\lambda_i$ become equal. This \sh{\sout{enforces} is because of} an ``eigenvalue repulsion'' in the Hessian matrices for the \sh{\sout{maxima and minima} random function}. This implies that a generic minima will be asymmetric, as their Hessians will have well separated eigenvalues.\SH{the eigenvalue repulsion is actually true fior any symmetric matrix, it isn't to do with minima/maxima}

Secondly,  looking at Eq.~\ref{Q} we see that if $\gamma$ is close to unity,  the second term in $Q$ becomes very large, making the integrand a steeper function of \sh{\sout{the $x_n$} $x_1$}. Conversely, if   $\gamma$ is small, $\nu$ can take larger values without dominating $Q$.  At larger values of $\gamma$ the power spectrum is \sh{\sout{effectively ``red''} peaked over a shorter range of $k$} and the random function is dominated by \sh{\sout{longer wavelength}} modes \sh{with a narrow range of wavelengths}. By constrast, when $\gamma$ is small the spectrum is \sh{\sout{``blue''} wider} and \sh{\sout{dominated by shorter} the function is affected by a wider range of} modes. In this case, it is more likely that \sh{\sout{extrema} minima} will be found at positive values of $\nu$. This behaviour is sketched in Fig~\ref{examples1} for illustrative one dimensional examples.   \SH{I don't think what we previously had is true. changes in $\gamma$ are to do with the bandwith of the power spectrum, not whether it is red or blue. In fact $\gamma$ isn't even defined for a blue spectrum, or an insufficiently red one because $\sigma_2$ becomes infinite.}

Thirdly, as $N$ increases, \sh{the polynomial $G$ gets more terms. This gives it a growing influence compared to $Q$, which makes the full probability density at minima prefer larger magnitude eigenvalues, $|\lambda_i|$. $x_1$ is the sum of the eigenvalues, so when its magnitude grows the $(x_1-\gamma \nu)^2$ term in $Q$ also grows. For minima, $x_1<0$, therefore large $|x_1|$ makes it less likely to find minima with $\nu >0$ (or $\nu <0 $ for maxima).} \sh{\sout{$Q$ will always grow, making it less likely to find minima and maxima on the ``wrong'' side of zero.}}\SH{i think this was confused before.}

Finally, because we are interested in the \sh{ranges $\nu \in (0 , \infty)$ and $\nu \in (-\infty , \infty)$}, we can simplify the integral Eq. \ref{PminIntegral} by performing the $\nu$ integral analytically. The only dependence in the integrand on $\nu$ is in the first two terms of $Q$, which further remains the same for all $N$:

\begin{equation}
\begin{split}
\int^\infty_{-\infty} d\nu \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} G \times e^{-Q} \sh{\prod_n dx_n} \\ 
=  \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} f(x_1, x_2 \ldots) \sh{\prod_n dx_n} \int^\infty_{-\infty} d\nu \,\exp\left(-\frac{\nu^2}{2}- \frac{(x_1- \gamma \nu)^2}{2(1-\gamma^2)}\right)\\
= \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} f(x_1, x_2 \ldots)  \sh{\prod_n dx_n}\int^\infty_{-\infty} d\nu \,\exp\left(-\frac{x_1^2}{2}- \frac{(\nu - \gamma x_1)^2}{2(1-\gamma^2)}\right)\\
= \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} f(x_1, x_2 \ldots) \sh{\prod_n dx_n}\exp\left(-\frac{x_1^2}{2}\right) \sqrt{2\pi}\sqrt{1-\gamma^2}\\
\end{split}
\end{equation}
\SH{Sorry, I've made these equations really ugly... previously we'd brought some $x_1$ dependence  out of the $x_1$ integral erroneously, so we need to keep that fixed, but it could look a lot nicer (even by just aligning the $=$ signs). I do think $\exp(f)$ looks nicer than $e^{f}$ though, or at least more readable.}

Similarly, 

\begin{equation}
\begin{split}
\int^\infty_0 d\nu \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} G \times e^{-Q} \sh{\prod_n dx_n}\\ 
= \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} f(x_1, x_2 \ldots) \sh{\prod_n dx_n}\exp\left(-\frac{x_1^2}{2}\right) \sqrt{\frac{\pi}{2}}\sqrt{1-\gamma^2} \left(1+\mathrm{Erf}\left(\frac{\gamma x}{\sqrt{2-2\gamma^2}}\right)\right)\\
\end{split}
\end{equation}

\sh{\sout{Although}} These analytic integrals are not always usable in what follows, \sh{but} when they are, they lighten the computational cost significantly.\SH{apparently you can't start a sentence with although. i've been doing it for years but a previous collaborator told me you can't and they were right}

\subsection{$N < 10$: Direct Evaluation  }

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{N369.png}
  \caption{The probability that a given minimum has $V > 0$ as a function of $\gamma$, for $N=3, 6, 9$. We can see that all the heuristics are obeyed: the probability decreases with $N$, and \sh{ $\gamma$ \sout{increases with $\sigma_2$}}. \re{Also -- we can smooth this, and use the LogListPlot (or ListLogPlot, or whatever it is ) command in mathematica}}
  \label{N6}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=\linewidth]{PVaryingWithN.png}
  \caption{A plot showing the value of $P(V>0|{\rm min})$ as a function of $N$ with $N\le10$, for $\gamma$ values $\frac{1}{5}, \frac{1}{3.5}$ and $\frac{1}{2}$ from top to bottom.\SH{Title needs changed}}
  \label{gamma}
\end{figure}

For relatively small values of $N$ we can compute the integrals without any further approximations.   In Figs.~\ref{N6} and \ref{gamma}, we present $P(V>0|{\rm min})$ as a function of $N$ and $\gamma$. As expected, if the potential is highly oscillatory (\sh{\sout{$\sigma_2$ large, or}} $\gamma$ small), $P(V>0|{\rm min})$  tends towards 0.5 --  an equal likelihood of any given \sh{\sout{extremum} minimum} being \sh{\sout{a maximum or a minimum} above or below $V=0$}. Conversely, if the potential is very smooth (\sh{\sout{$\sigma_2$ small;}} $\gamma$ very large), $P(V>0|{\rm min})$  tends towards 0. Moreover, for any given $\gamma$, $P(V>0|{\rm min})$ decreases with increasing $N$. 
 
%\begin{table}[h!] \label{Data}
%  \begin{center}
%    \caption{The probability that a given minimum has $\phi > 0$ as a function of $\sigma_2$. Values given for $N=4,5,6$ are exact; for larger values they are approximate (see the text; errors are on the order of the third nonzero digit after the decimal).}
%    \label{tab:table1}
%    \begin{tabular}{c|c|c} % <-- Alignments: center/center/center (l and r for left and right if needed)
%      $\textbf{N}$ & $\sigma_2$ & $p_{min}$\\
%      \hline
%      4 & 10 & 0.39141\\
%      5 & 10 & 0.37667\\
%      6 & 10 & 0.36320\\
%      7 & 10 & 0.35210\\
%      8 & 10 & 0.34227\\
%      9 & 10 & 0.33159\\
%      10 & 10 & 0.32151\\
%      \hline
%      4 & 3.5 & 0.21520\\
%      5 & 3.5 & 0.17886\\
%      6 & 3.5 & 0.15274\\
%      7 & 3.5 & 0.13599\\
%      8 & 3.5 & 0.11711\\
%      9 & 3.5 & 0.10093\\
%      10 & 3.5 & 0.08729\\
%      \end{tabular}
%      \quad
%      \begin{tabular}{c|c|c} 
%      $\textbf{N}$ & $\sigma_2$ & $p_{min}$\\
%      \hline
%      4 & 2 & 0.066638\\
%      5 & 2 & 0.043103\\
%      6 & 2 & 0.030844\\
%      7 & 2 & 0.02013\\
%      8 & 2 & 0.01307\\
%      9 & 2 & 0.00845\\
%      10 & 2 & 0.00544\\
%      \hline
%      4 & 1.25 & 0.001526\\
%      5 & 1.25 & 0.000327\\
%      6 & 1.25 & 6.53328 $\times 10^{-5}$\\
%      7 & 1.25 & 2.09673 $\times 10^{-5}$ \\
%      8 & 1.25 & 3.97235  $\times 10^{-6}$ \\
%      9 & 1.25 & 7.13489  $\times 10^{-7}$ \\
%      10 & 1.25 & 1.20253  $\times 10^{-7}$ \\
%     \end{tabular}
%  \end{center}
%\end{table}

%As can be seen, the probability that a given minimum has $\phi > 0$ decreases with increasing $N$. For a constant $\sigma_0$ and $\sigma_1$, the probability also decreases for increasing $\sigma_2$. Both of these general trends are to be expected: as $N$ increases, there are more conditions that must be simultaneously met for a point to be a minimum, hence the probability decreases. Also, since $\sigma_2$ is the average of the second derivative of the function, as $\sigma_2$ increases the function gets more and more turbulent. This means there are both more maxima and more minima, and the probability increases.

\subsection{$10 < N < \sim30$: Gaussian Approximations to the Integral}

For $N>10$, direct calculation becomes very resource-intensive. To proceed, we \sh{instead} first select a value of the field $\nu$, then find the values of $x_n$ that maximize the integrand in Eq. \ref{DensityOfPeaks}. We then approximate the integrand as a Gaussian about this maximum likelihood point \sh{(note that although the function $V$ is a Gaussian random function, the derived probability density function at minima is not an exact Gaussian due to the $G$ polynomial)}:

\begin{align*}
\begin{split}
\langle n_{\rm min}(\nu)\rangle d\nu &= A \int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} G(\nu) \times e^{-Q(\nu)} \prod_n dx_n d\nu\\
&=A\int_{\lambda_1 \geq \lambda_2 \geq \lambda_3 \ldots \geq 0} e^{\mathrm{log}G-Q} \prod_n dx_n d\nu\\
&\approx \int_{-\infty}^{\infty} \exp\left(-\frac{1}{2}\sum_{l,m}x_lH_{lm}x_m\right)  \prod_n dx_n d\nu = \sqrt{\frac{(2\pi)^N}{\mathrm{det} H(\nu)}}d\nu
\end{split}
\end{align*}

\begin{figure} 
  \centering
  \includegraphics[width=\linewidth]{Comparison.png}
  \caption{A comparison of $P(\nu)d\nu$ with the Gaussian approximation, at specified values of $\nu$ ($N=8$ \SH{what $\gamma$?}). Blue line corresponds to the Gaussian results, while the orange line corresponds to the exact integral. Horizontal axis is $\nu$ while the vertical axis is (unormalized) \sh{probability density}. As expected, because the Gaussian integral integrates to infinity while the exact integral does not, it yields a larger likelihood. \SH{The $(N-1)^\alpha$ correction takes this into account.}}
  \label{Comparison}
\end{figure}

\noindent where $H_{lm}$ is the Hessian \sh{of the integrand with respect to the $x_n$ variables} at the \sh{\sout{point} value of the potential} being considered\sh{\sout{, and we no longer have an integral over $\nu$}}. This produces a list of points for each value of $\nu$. Fitting these points to a function and integrating that for the region with $\nu > 0$, we get an approximate result of the integral.\SH{Why fit to a function and not just integrate directly? I think I've been told this before, but I've forgotten.} \lfl{I feel like there's a more technical term for this, don't know what though.} This approximate value can then be compared against the exact value. The Gaussian approximation turns out to be very good (see Fig. \ref{Comparison}). The Gaussian integral can be more efficiently computed than Eq. \ref{DensityOfPeaks}, which lets us evaluate it up to $N\approx30$.
\SH{Do we want to also show the even \emph{better} result when we look at the ratios? Or has figure 4 already done this? Also, we could include here the comparison to $(N-1)^\alpha$ to show we understand why the gaussian approximation misses. We could also use the $(N-1)^\alpha$ corrected Gaussian approximation as the default if we want. Finally, I think we could do with a more thorough examination of when/where this approximation works and where it doesn't, as a function of $N$ and $\gamma$.}

\begin{figure} 
  \centering
  \includegraphics[width=\linewidth]{PVaryingWithNGaussian.png}
  \caption{(Left) The probability of a minimum above $\nu=0$, calculated using the Gaussian approximation. The lines correspond to $\gamma = \frac{1}{10}, \frac{1}{5}, \frac{1}{3.5}, \frac{1}{2.5}$ respectively from top to bottom. Compare Fig. \ref{gamma} \SH{Why can't we overplot the exact result up to $N=10$ on both LHS and RHS}. (Right) The same data, plotted using a different y-axis ($Log$ here is to base 10 \SH{Richard's suggestion of just plotting $P(V>0)$ with a logarithmic axis is a good one}). All four lines yield a logarithm well above $-500$ \sh{\sout{at} if extrapolated to} $N=100$. The cutoff at which $p(V>0|{\rm min}) \sim 10^{-500}$, \sh{\sout{with $\sigma_0$ and $\sigma_1$ normalized to 1,  is $\sigma_2 \approx 1.08$}}\SH{can we just write the $\gamma$ value? Titles need $F$ changed to $V$ in the plots}.}
  \label{PVaryingWithNGaussian}
\end{figure}
  
The plot of the logarithms of these results against the number of dimensions is surprisingly straight (Fig. \ref{PVaryingWithNGaussian}). Although it is not conclusive,\footnote{Per Figure 4 of \cite{Yamada2018}, it is possible the behavior of the function changes at $N >100$.} it is suggestive.\SH{Can we overplot the results for the exact integral to show how well they match?}

\subsection{$\sim 30 < N < \sim100$: Approximations using the $\nu=0$ point}

The Gaussian approximation described in the previous subsection requires the integral to be computed for several different values of $\nu$. If we use only the result at the $\nu=0$ point, we get a result that is much larger than the exact results, but it remains at a nearly-constant ratio to it (see Fig. \ref{sigma2=2v=0})\SH{We can (appproximately) predict this ratio, and I think we should plot the ``corrected'' value for the approximation. I think we should also have a plot like Fig 6, but for a few values of $\gamma$ and we should include comparisons of all three methods on it.}. Using this, we confirm that the straight line behavior holds up to $N = 100$.

\begin{figure} 
  \centering
  \includegraphics[width=\linewidth]{sigma2=2v=0.png}
  \caption{A comparison of the Gaussian approximation evaluated at the $\nu=0$ point to the full integral. The blue line corresponds to the $\nu=0$ results, while the orange line corresponds to the Gaussian results (Section 3.3). Horizontal axis is $N$ while the vertical axis is the $Log_{10}$ of the probability. The $\nu=0$ results are always larger than the actual results, but the ratio of their logarithms is approximately constant.}
  \label{sigma2=2v=0}
\end{figure}

\SH{change to $\gamma?$}Based on these calculations, we conclude that the value of $\sigma_2$ for which $P(V>0|{\rm min})$ drops to $\sim 10^{-500}$ at $N = 100$ is about $1.08$. In other words, there will be plenty of minima in the landscape that could correspond to our universe if the landscape has $\sigma_2 \gtrapprox 1.08$ (with $\sigma_0$ and $\sigma_1$ normalized to 1).

%We have tried two ways to do this.

%The first way is to extrapolate from the data in Table \ref{Data}. In Fig. \ref{Log-Linear}, we present a log-linear plot of the probability against the dimension for the three values of $\sigma_2$. The resulting lines are surprisingly straight. There is a noticeable kink in both the $\sigma_2 = 3.5$ and $\sigma_2=2$ case between $N=6$ and $N=7$. This is a numerical effect -- it is where the exact results end and the numerical ones begin. If the trend is robust, then for the case of $\sigma_2=2$, the probability at $N=100$ is of the order of $p \sim 10^{-42}$. Since there are $\sim 10^{500}$ stationary points in the landscape,\cite{Douglas} this probability, while small, still leads to there being $\sim 10^{458}$ minima that might correspond to our universe.

%We checked this by running the numerical integration for $N=6$ as well. Replacing the exact result with the numerical one, the kink moves to the left. The kink indicates that the numerical results are consistently overestimating the true probability, but that is not surprising because the numerical results rely on the trapezoidal rule. The trapezoidal rule overestimates the result when the integral is concave up, and underestimates it when it is concave down. The final probability is the area under the curve for $\phi > 0$ divided by the area under the curve for all $\phi$. From Fig. \ref{Likelihood}, for $\phi > 0$ the curve is concave up, so the trapezoidal rule overestimates the numerator, and accordingly the probability as well. This trend holds for higher $N$ as well, since for higher $N$ the plot moves to more negative values.

%\begin{figure}
%  \centering
%  \includegraphics[width=\linewidth]{LikelihoodPlot.png}
%  \caption{A plot of the likelihood for $\sigma_2=2, N=6$. The final probability desired is the area under the curve for $\phi > 0$ divided by the area unde the curve for all $\phi$. For the region $\phi > 0$, the curve is concave upwards, so the trapezoidal rule overestimates its value. Accordingly, the numerator and probability are also overestimated.}
%  \label{Likelihood}
%\end{figure}

%The other way is to with ratios of the peak likelihood. The idea is to take the logarithm of the integrand in Eq. \ref{DensityOfPeaks}, which results in a function $G(x, y, z, \ldots)$, and then find the values of $x, y, z \ldots$ which maximizes this integrand. This corresponds to the single most likely point. Having done this, we can compute the ratio of this maximum vs. the point $\phi = 0$, which is a number much more easily calculated. The result in 4D is show in Fig. \ref{LogRatioNoFit}. It's obvious that the blue line is not a good fit of the orange line, but it takes the same shape. Since string theory predicts a truly gargantuan number of stationary points -- on the order of $10^{500}$ -- being off by even $\sim100$ orders of magnitude is not necessarily fatal.

%\begin{figure}
%  \centering
%  \includegraphics[width=\linewidth]{LogRatioNoFit.png}
%  \caption{Plot of the logarithm of the ratio of likelihood for the overall peak to the point $\phi = 0$ as a function of $\gamma$, as well as the analytic results, in 4 dimensions. The blue line is the logarithm of the ratio of likelihood, the orange line is the analytic results.}
%  \label{LogRatioNoFit}
%\end{figure}

\section{Implications for Multiverse Cosmology}

\SH{I think a section like this would be nice... everything up to now is an explanation of why we should trust our $P(V>0|{\rm min})$ results... now we should play with them and say interesting stuff about them! Richard's suggestion of stuff around how steep, or not steep, minima would be at different values of $V$ would be good (although we should check that Vilenkin etc haven't already done it in one of their papers).}

\SH{We could also put the contour plot here and put a line in it to show where one would expect at least one minimum above $V=0$.}
  
\re{I would move from looking at the abstract problem in the previous section to the specific implications for cosmology in this section; and how $P(\Lambda)$ depends on $\sigma_2$}

\lfl{Dunno what to put here anymore}

\section{Conclusion}
\SH{Change to $V$ and $\gamma$}

\SH{Has any paper pointed out that $P(V>0|{\rm min})$ is actually really small in the first place, maybe that itself isn't understood yet. And then we say exactly how small, and find the range of $N$ and $\gamma$ for which it is big enough that at least one above water minima is expected. We could also put the dimensions back in for $V$ (e.g. make $\sigma_0$ equal to the Planck vacuum energy)  and say something about how probable/improbable it is that $V$ gets to the value we observe?}

\SH{For ``future work'' we could also mention that we aren't necessarily in a minimum, we're just at a point where $\eta$ is below some threshold, which would require a slightly different analysis, in particular we wouldn't limit ourselves to extrema and thus would lose the $\prod |\lambda|$ term in the pdf, thus favouring smaller eigenvalues a lot more. There's heaps of things that could be explored/asked about...}


We have presented results for the statistics of stationary points at a given function value as a function of $N$ and $\sigma_2$. The numbers confirm the intuitive expectation that the probability of a given extremum with $V > 0$ being a minimum decreases as $N$ increases or as $\sigma_2$ decreases. We are able to calculate precise values for the probability up to $N=10$. Above this dimension, evaluating Eq. \ref{DensityOfPeaks} is computationally prohibitive, but we find that Eq. \ref{DensityOfPeaks} is well-approximated by a Gaussian integral. Using this approximation, we are able to estimate the probability up to $N \approx 30$. For $N>30$, we use only the $\nu=0$ point, which although much smaller than the actual results, remains in roughly constant ratio with it. The final probability as a function of $N$ is well-approximated by a straight line when plotted on a logarithmic scale for various values of $\sigma_2$ (Fig. \ref{PVaryingWithNGaussian}). If this trend holds to larger values of $N$, we estimate that the value of $\sigma_2$ for which $P(V>0|{\rm min})$ drops to $\sim 10^{-500}$ is approximately $1.08$.

The results of this paper establish the number of vacua that could correspond to our universe in string theory, but does not treat the distribution of eigenvalues -- and accordingly the duration of slow-roll inflation -- that could arise from these vacua. An analysis of this will be the focus of future work.

\section{Appendix}
\subsection{Density of peaks for $N=4$} 
For $N=4$, the full result of the integral Eq. \ref{DensityOfPeaks} is: \lfl{This needs to be re-checked, there shouldn't be $x$ in the result. Computer is busy with the $N=100$ calculations for $\sigma_2=1.08$, will do it later.}

\begin{equation}
\begin{split}
N = \frac{1}{214990848} \mathrm{Exp}\frac{9x^2\sigma_1^4 + 2\nu x \sigma_0 \sigma_1^2 \sigma_2 - (v^2+10x^2)\sigma_0^2\sigma_2^2}{-2\sigma_1^4+2\sigma_0^2\sigma_2^2}\\\bigg{(}80x - 1610e^{3x^2}+128x^3+418e^{3x^2}x^3+4e^{4x^2}\sqrt{\pi}(3+48x^2+64x^4)\mathrm{Erf}\bigg{[}\sqrt{\frac{3}{2}}x\bigg{]}\\
-486e^{\frac{9x^2}{2}}\sqrt{6\pi}x^2\mathrm{Erf}\bigg{[}\sqrt{\frac{3}{2}}x\bigg{]}+81e^{\frac{9x^2}{2}}\sqrt{6\pi}x^4\mathrm{Erf}\bigg{[}\sqrt{\frac{3}{2}}x\bigg{]}\bigg{)}
\end{split}
\end{equation}

The higher-dimensional results take the same form: an overall exponential multiplied by a product of polynomials and error functions; however they are massive (for example, in 5D there are some eight hundred terms).

\subsection{$p(V|s.p.)$ for $N=2$}
\lfl{As long as the reference above to the difference with Yamada \& Vilenkin is commented out, this appendix isn't needed either}
The full expression for $p(V|s.p.)$ in 2D is:

\begin{equation}
\begin{split}
p(V|s.p.)=\frac{\sqrt{\pi}}{4\sqrt{4\gamma^2-6}}\bigg{[}\mathrm{Exp}\frac{V^2}{2(\gamma^2-1)}\bigg{(}2\mathrm{Exp}\bigg{(}\frac{V^2\gamma^2}{e^6-10\gamma^2+4\gamma^4}\bigg{)}\sqrt{\gamma^2-1}\\
-\mathrm{Exp}\bigg{(}\frac{V^2\gamma^2}{2-2\gamma^2}\bigg{)}(V^2-1)(\gamma^2-1)\gamma^2\sqrt{\frac{2\gamma^2-3}{1-\gamma^2}}\bigg{)}\bigg{]}
\end{split}
\end{equation}

\noindent where $\gamma = \sigma_1^2/\sigma_0\sigma_2$. This result can be derived using a similar method as to derive Eq. \ref{DensityOfPeaks}, but by relaxing the requirement that the smallest eigenvalue is greater than zero.

\begin{thebibliography}{99}
\bibitem{Planck2018} Planck Collaboration 2018 results. Submitted to \emph{Astronomy \& Astrophysics}.
\bibitem{DES} Dark Energy Survey year 1 results. arXiv:1802.05257
\bibitem{Linde1986} A. D. Linde, Phys. Lett. B 175, 4, 395--400, 1986
\bibitem{Adshead2007} P. Adshead, R. Easther, and E. A. Lim, Phys. Rev. D 79, 063504, 2009
\bibitem{Guth2001} A. Guth, arXiv: astro-ph/0101507
\bibitem{Susskind2003} L. Susskind, arXiv:hep-th/0302219
\bibitem{Bousso2000} R. Bousso and J. Polchinski, Journal of High Energy Physics, 06, 006, 2000
\bibitem{Agrawal2018} P. Agrawal, G. Obied, P. J. Steinhardt, C. Vafa, Physics Letters B, 784, 271--276, 2018
\bibitem{GRF1} A. Masoumi, A. Vilenkin and M. Yamada, Journal of Cosmology and Astroparticle Physics, 05:053, 2017
\bibitem{GRF2} A. Masoumi, A. Vilenkin and M. Yamada, Journal of Cosmology and Astroparticle Physics, 12:035, 2017
\bibitem{GRF3} T. Bjorkmo and M.C.D. Marsh, Journal of Cosmology and Astroparticle Physics, 02:037, 2018
\bibitem{Aazami2006} A. Aazami and R. Easther, Journal of Cosmology and Astroparticle Physics (0603:013), 2006
\bibitem{Bray2007} A. J. Bray and D. S. Dean, Physical Review Letters, 98, 150201, 2007
\bibitem{Easther2006} R. Easther and L. McAllister, Journal of Cosmology and Astroparticle Physics 05:018, 2006
\bibitem{Frazer2011} J. Frazer and A. R. Liddle, Journal of Cosmology and Astroparticle Physics 02:026, 2011
\bibitem{Henry2009} S.-H. Henry Tye, J. J. Xu and Y. Zhang, Journal of Cosmology and Astroparticle Physics 04:018, 2009
\bibitem{Marsh2013} M.C.D. Marsh, L. McAllister, E. Pajer and T. Wrase, Journal of Cosmology and Astroparticle Physics 11:040, 2013
\bibitem{Agarwal2011} N. Agarwal, R. Bean, L. McAllister and G. Xu, Journal of Cosmology and Astroparticle Physics 09:002, 2011
\bibitem{Yang2012} I.S. Yang, Physical Review D, 86, 103537, 2012
\bibitem{Masoumi2016} A. Masoumi and A. Vilenkin, Journal of Cosmology and Astroparticle Physics 03:054, 2016
\bibitem{Yamada2018} M. Yamada and A. Vilenkin, Journal of High Energy Physics 2018: 29, 2018
\bibitem{Fyodorov2004} Y. V. Fyodorov, Physical Revew Letters, 92, 240601, 2004
\bibitem{Douglas2004} M. R. Douglas, B. Shiffman and S. Zelditch, Communications in Mathematical Physics, 252, 325, 2004
\bibitem{Douglas2006} M. R. Douglas, B. Shiffman and S. Zelditch, Communications in Mathematical Physics, 265, 617, 2006 
\bibitem{Fyodorov2007} Y. V. Fyodorov and I. Williams, Journal of Statistical Physics, 129, 5, 2007
\bibitem{Fyodorov2012} Y. V. Fyodorov and C. Nadal, Physical Review Letters, 109, 167203, 2012
\bibitem{Fyodorov2018} Y. V. Fyodorov, P. L. Doussal, A. Rosso, C. Texier, Annals of Physics, 397, 2018
\bibitem{Ros2019}  V. Ros, G. B. Arous, G. Biroli and C. Cammarota, Physical Review X 9, 011003
\bibitem{Dean2008} D. S. Dean and S. N. Majumdar, Physical Review E., 77, 041108, 2008
\bibitem{Majumdar2009} S. N. Majumdar, C. Nadal, A. Scardicchio, and P. Vivo., Physical Review Letters, 103, 220603, 2009
\bibitem{Bachlechner2014} T.C. Bachlechner, Journal of High Energy Physics, 2014: 54, 2014
\bibitem{Battefeld2012} D. Battefeld, T. Battefeld, S. Schulz, Journal of Cosmology and Astroparticle Physics, 06:034, 2012
\bibitem{Fyodorov2013} Y. V. Fyodorov, Markov Processes Relat. Fields, 21, 483--51, 2015
\bibitem{Masoumi2017} A. Masoumi, M. Yamada and A. Vilenkin, Journal of Cosmology and Astroparticle Physics, 07:003, 2017
\bibitem{Easther2016} R. Easther, A. Guth and A. Masoumi, arXiv:1612.05224 (2016)
\bibitem{Kac1943} M. Kac, \emph{Bull. Amer. Math. Soc.}, 43, 314–320, 1943
\bibitem{Rice1945}  S. O. Rice, \emph{Bell System Tech. J.}, 24, 46--156, 1945
\bibitem{BBKS} J. M. Bardeen, J. R. Bond, N. Kaiser, and A. S. Szalay, Astrophysical Journal, Astrophysical Journal, vol. 304, page 15-61 (1986)
\bibitem{Goldstein} See e.g. H. Goldstein, C. P. Poole, and J. L. Safko, \emph{Classical Mechanics 3rd ed.}, Pearson (2001)
\bibitem{VEGAS} G. P. Lepage, Journal of Computational Physics 27, 192, 1978.
\bibitem{GSL} B. Gough, \emph{GNU Scientific Library Reference Manual - Third Edition, 3rd ed.} (Network Theory Ltd., 2009).
\bibitem{Douglas2003} M. R. Douglas, Journal of High Energy Physics, 05:046, 2003
\end{thebibliography}

\end{document}
